import{_ as m,r,o as h,c as u,w as a,a as t,b as s,d as e,e as y}from"./index.578cad93.js";const w={},b=s(" I followed through a tutorial by Alexis Cook that went through the process of completing in a Kaggle competition. The practice competition uses a passenger dataset from the Titanic. The dataset specifies a passengers attributes such as age, sex, ticket class, etc. The dataset also indicates if they survived or not. The challenge is to create a prediction model that predicts whether a given passenger would survive. "),v=s("Going into the directory with the training data, testing data and example prediction .csv files."),k=s("Assigning training data and testing data. Also displaying the 1st five passengers with their respective data for each dataset."),I=s("Testing if gender_submission.csv prediction of all female passengers survived and all male passengers died. Since the numbers are 74% women and 18% men we can say that this prediction is significantly inaccurate. "),x=s("Using the Random Tree Model to make predictions based on a given passenger's data."),T=s(" After following the tutorial from Alexis Cook we built a prediction model that uses a random forest model. A random forest model produces trees that take each attribute of the passenger's data into account. The model uses a majority vote with the outcome of the trees to predict if the passenger survived. The model we built only accounted for the passenger's class, sex, amount of siblings and spouses, amount of parents and children. This model produced a score of .77511 accuracy. "),$=e("h1",{style:{color:"white"}},"Reference:",-1),B=e("a",{href:"https://www.kaggle.com/code/alexisbcook/titanic-tutorial/notebook"},"Alexis Cook's Titanic Kaggle Tutorial (Click me)",-1);function N(g,f){const i=r("n-h2"),o=r("n-image"),n=r("n-space"),l=r("n-gi"),d=r("n-grid");return h(),u(n,{vertical:""},{default:a(()=>[t(d,{cols:"1",responsive:"screen","x-gap":100},{default:a(()=>[t(l,null,{default:a(()=>[t(n,null,{default:a(()=>[t(i,{style:{color:"white"}},{default:a(()=>[b]),_:1}),t(i,{style:{color:"white"}},{default:a(()=>[v]),_:1}),t(o,{src:"assets/KaggleTutorialSnip1.png",width:"800"}),t(i,{style:{color:"white"}},{default:a(()=>[k]),_:1}),t(o,{src:"assets/KaggleTutorialSnip2.png",width:"800"}),t(i,{style:{color:"white"}},{default:a(()=>[I]),_:1}),t(o,{src:"assets/KaggleTutorialSnip3.png",width:"800"}),t(i,{style:{color:"white"}},{default:a(()=>[x]),_:1}),t(o,{src:"assets/KaggleTutorialSnip4.png",width:"800"}),t(i,{style:{color:"white"}},{default:a(()=>[T]),_:1}),t(o,{src:"assets/KaggleTutorialSnip5.png",width:"800"})]),_:1})]),_:1})]),_:1}),$,B]),_:1})}const C=m(w,[["render",N]]),A={},z=s("Motivation"),E=s(" For my Data Mining class we were assigned to use a dataset from "),H=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://www.kaggle.com/datasets/maricinnamon/caltech101-airplanes-motorbikes-schooners"},"caltech101 airplanes-motorbikes-schooners competition",-1),D=s(" . The objective was to derive a model to predict the classes of each image and through experimentation increase the performance of the model. This is my attempt to increase the performance of the model. "),F=s("Source code and background"),M=s(" The dataset used is from "),j=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://www.kaggle.com/datasets/maricinnamon/caltech101-airplanes-motorbikes-schooners"},"caltech101 airplanes-motorbikes-schooners competition",-1),K=s(" . It contains 3 folders of airplanes, motorbikes and schooners. To start the assignment, I looked online for source code that implemented Convolution Neural Networks for the Machine Learning process while being able to process the images from the dataset. That\u2019s when I stumbled upon Geeks for Geeks "),R=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://www.geeksforgeeks.org/python-image-classification-using-keras/"},"\u201CPython Image Classification using Keras\u201D",-1),S=s(" article. In the article they use the Keras library for processing the images, creating, and training a model, and predicting the images. However, the approach they used was an 80:20 split for their training and testing. Additionally, it was only concerned with classification for 2 classes."),q=s(" I decided to implement a part of the source code from this article and would look for source code that implemented prediction and training for more than 2 classes. I found a medium article "),P=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720"},"\u201CTutorial on using Keras flow_from_directory and generators\u201D",-1),G=s(" , that gave a tutorial for the training and prediction process. This article also only classified 2 classes for cats and dogs, but implemented a training, validation, and test datasets using Keras model again. "),W=s(" In both models they used \u201Cbinary cross entropy\u201D for loss, \u201Crmsprop\u201D for optimization, and their metrics were \u201Caccuracy\u201D. Binary cross entropy\u2019s loss function is great for binary classification and a great article I found is medium\u2019s "),O=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a"},"\u201CUnderstanding binary cross-entropy / logs loss: a visual explanation\u201D",-1),L=s(" . To give a brief explanation what loss functions do in convolutional neural networks. The output is evaluated at the end of forward propagation to evaluate how far off the model is from the desired goal/output of the given input by using a given loss function. In the case of using multiple classes binary cross entropy isn\u2019t best suited. "),U=s(" Therefore, in my model I used categorical cross entropy as the loss function. This loss function is ideal as it takes the probabilities of each class and adjusts the weights in favor of the actual class. As opposed to binary cross entropy which is concerned about the probability of the class being a class or not. This means it isn\u2019t really concerned about identifying other images if it can classify one and say it is or is not that class. "),V=s(" They used \u201Crmsprop\u201D for optimization and after reading over an article from TowardsDataScience, "),J=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b"},"\u201CA look at gradient descent and rmsprop optimizers\u201D",-1),Q=s(" it seems that they used this in order to increase the learning rate with larger steps without worrying about overstepping during the descent. For my model I decided to use \u201Csgd\u201D or Stochastic Gradient Descent. I read an article by geeks for geeks "),X=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/"},"SGD",-1),Y=s(" on how the optimizer works. I simply used this to try and derive different results that could be compared to using rmsprop. "),Z=s(' Lastly, their model used simply \u201Caccuracy\u201D as their metric, which gives the difference between the prediction and the actual. However, in the case of multi-classification, we need a metric that gives us a sum of the correctly matched/total images. This is more realistic since we aren\u2019t measuring the probability of it being a class, but if the class matched was correct or not. Therefore, I used "categorical accuracy" as my metric for the model. '),ee=s("What is forward and backward propagation?"),te=s(" Forward propagation is passing input data through layers that reach an output layer. The input goes through layers known as \u201CNode layers\u201D, and they have set weights on each node that performs an operation on the input. These layers are the hidden layers of a neural network and for this network to exist it must possess at least one hidden layer. Once the input reaches the output layer the output is then evaluated based on how far off the desired output was. After the output is assessed, backward propagation begins. Backward propagation goes backward going through each layer and adjusting each node of each layer in hopes to close the gap between the output and the desired output or decrease the \u201Closs\u201D. "),ae=s("Experiments & results"),se=s(" In the experiments I would adjust the hyper parameters of the model to derive different results. Each result was recording on an "),oe=e("a",{style:{color:"#e07a5f"},href:"assets/Assignment_1_Experiment_Data.xlsx",download:""},"excel sheet",-1),ie=s(" with each row representing an experiment. The columns representing the results: training time (seconds), accuracy, batch size, epochs, node layers, loss(percent). "),ne=s(" This is the result of the initial state of the program, after applying changes to make it work with the new dataset and a 60:20:20 ratio for training, validation, and testing. Through out the experiments I would monitor the training vs. validation performance to make sure that over fitting wouldn't occur and if so I would report it in the experiment. "),re=s(" For accuracy I would use this prediction output for my accuracy that I calculated myself. Note that the above image does have an accuracy for training and validation, but I used the prediction model output as it was tested on the test dataset. Also my accuracy throughout these experiments never changed as I was unsuccessful in even changing the result of the accuracy. "),le=s(" From the initial state to the 1st experiment I added an extra layer to the training model to see if this could possibly improve performance. This did not improve performance and actually resulted in a negative effect in loss and accuracy. "),de=s(" In this experiment I kept the initial state and doubled the epochs from 10 to 20 in hopes that more runs in the training model would result in increased performance. This actually did increase the accuracy, but at the cost of increased training time. However, according to the line graph my training had over fitting occur. "),ce=s(" In this experiment I kept the initial state and doubled the batch size from 16 to 32. The result of this was a slightly faster training time. The training model did take a slight decrease in accuracy and a significant increase in loss. "),he=s("Conclusion"),ue=s(" To conclude this blog I found that increasing the epochs did increase the accuracy of the model. However, it did result in over fitting as the training performance exceeded the validation performance. Increasing the layers did not result in better performance and actually increased loss and accuracy significantly. Doubling the batch size also did not yield any not worth results. Lastly, the initial state of the model from the geeks for geeks tutorial also suffered from over performance. This means experiment #1's result yielded the best outcome. "),ge=s("Contributions"),fe=e("thead",null,[e("tr",null,[e("th",null,"Contribution"),e("th",null,"The value of the contribution"),e("th",null,"Technical Challenges")])],-1),pe=e("tbody",null,[e("tr",null,[e("td",null,"Explanation of forward and backward propagation"),e("td",null,"I explained at a high level the process of forward and backward propagation in my own words."),e("td",null,"No technical difficulties")]),e("tr",null,[e("td",null,"Model metric"),e("td",null,"I added into the model a different metric rather than using accuracy I used categorical accuracy."),e("td",null," No technical difficulties ")]),e("tr",null,[e("td",null,"Experiment #1"),e("td",null," In this experiment I changed the hyperparameter of neural layers from 3 to 4 and had the output filters increase and then decrease in the layers. The outcome of this experiment resulted in a decrease in accuracy and increase in loss. Showing that I was possibly filtering the inputs too much to the point that nothing accurate could be derived from it. "),e("td",null,"No technical difficulties")]),e("tr",null,[e("td",null,"Experiment #2"),e("td",null," In this experiment I changed the hyperparameter of epochs from 10 to 20. The outcome of this experiment resulted in a increase in accuracy and decrease in loss. Show that increasing the model epochs correlate to an increased performance, but longer training time. "),e("td",null,"No technical difficulties")]),e("tr",null,[e("td",null,"Experiment #3"),e("td",null," In this experiment I changed the hyperparameter of batch sizes from 16 to 32. The outcome of this experiment resulted in a decrease in accuracy and increase in loss. A positive of this is the training time did decrease. This shows that the batch size increase in this scenario had little to no difference on the model. "),e("td",null,"No technical difficulties")]),e("tr",null,[e("td",null,"Converted the binary classification model to a multi-classification model"),e("td",null,"I was able to convert the training model from the geeks for geeks tutorial to a multi-classification model."),e("td",null,' I spent 3 hours trying to figure out how to solve an error I would get when using "categorical cross-entropy". Turns out the model was pushing out only 1 node and not the same amount of nodes as there were classes in the training model. It also explained why my accuracy was also very consistent. ')])],-1),me=s("Reference"),_e=e("thead",null,[e("tr",null,[e("th",null,"Reference"),e("th",null,"How I used the reference"),e("th",null,"What value I made over the reference")])],-1),ye=e("tbody",null,[e("tr",null,[e("td",null,'"https://www.kaggle.com/datasets/maricinnamon/caltech101-airplanes-motorbikes-schooners"'),e("td",null,"I used their dataset for my experiments"),e("td",null," I converted each of these folders into a training, validation, and test folder. I used a 60:20:20 ratio respectively. Each folder also needed to have a subfolder for motorbikes, airplanes, and schooners that contained their respective images. This was done so the Keras model would work with this dataset. This took me an hour to implement. ")]),e("tr",null,[e("td",null,'"https://www.geeksforgeeks.org/python-image-classification-using-keras/"'),e("td",null,"I used their training model in my code, but not their data generator and prediction code."),e("td",null,"This allowed me to experiment with the hyper parameters of the data.")]),e("tr",null,[e("td",null,'"https://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720" '),e("td",null,"I used their data generator and prediction code."),e("td",null,"This allowed me to derive results from the experiments I conducted on the model using various hyper parameters.")]),e("tr",null,[e("td",null,'"https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a" '),e("td",null,"I read this article to give me an understanding of the binary cross entropy loss function."),e("td",null,"I used what I learned from this article to explain in my blog why using this loss function wouldn't work for multi-classification. ")]),e("tr",null,[e("td",null,'"https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b" '),e("td",null,"I read this article to give me an understanding of rmsprop optimizers."),e("td",null,"I got an understanding that the reason the code I referenced used and rmsprop optimizer was to allow for greater learning rates. This optimizer allows for greater learning rates without the fear of over stepping on the descent. ")]),e("tr",null,[e("td",null,'"https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/" '),e("td",null,"I read this article to give me an understanding of stochastic gradient descent."),e("td",null,"I was able to derive that rmsprop optimizer and sgd optimizer are very similar, but rmsprop allows for a faster learning rate. I would like to experiment with these different optimizer to see if I get different results. ")])],-1),we=s("Downloadable Code:"),be=e("a",{style:{color:"#e07a5f"},href:"assets/Reference_Code.ipynb",download:""},"Original code from geeks for geeks",-1),ve=e("a",{style:{color:"#e07a5f"},href:"assets/data-mining-assignment-1.ipynb",download:""},"My implementation with experiments",-1);function ke(g,f){const i=r("n-h1"),o=r("n-h2"),n=r("n-image"),l=r("n-table"),d=r("n-space"),c=r("n-gi"),p=r("n-grid");return h(),u(d,{vertical:""},{default:a(()=>[t(p,{cols:"1",responsive:"screen","x-gap":100},{default:a(()=>[t(c,null,{default:a(()=>[t(d,null,{default:a(()=>[t(i,{style:{color:"#e4572e"}},{default:a(()=>[z]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[E,H,D]),_:1}),t(i,{style:{color:"#e4572e"}},{default:a(()=>[F]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[M,j,K,R,S]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[q,P,G]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[W,O,L]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[U]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[V,J,Q,X,Y]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Z]),_:1}),t(i,{style:{color:"#e4572e"}},{default:a(()=>[ee]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[te]),_:1}),t(i,{style:{color:"#e4572e"}},{default:a(()=>[ae]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[se,oe,ie]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[ne]),_:1}),t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/initial_state_loss_graph.png",width:"800"}),t(o,{style:{color:"white"}},{default:a(()=>[re]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[le]),_:1}),t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/Extra_layer.png",width:"800"}),t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/experiment_1_loss_graph.png",width:"800"}),t(o,{style:{color:"white"}},{default:a(()=>[de]),_:1}),t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/double_epochs.png",width:"800"}),t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/experiment_2_loss_graph.png",width:"800"}),t(o,{style:{color:"white"}},{default:a(()=>[ce]),_:1}),t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/double_batch.png",width:"800"}),t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/experiment_3_loss_graph.png",width:"800"}),t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/all_experiments.png",width:"800"}),t(o,{style:{color:"white"}},{default:a(()=>[t(i,{style:{color:"#e4572e"}},{default:a(()=>[he]),_:1}),ue]),_:1}),t(i,{style:{color:"#e4572e"}},{default:a(()=>[ge]),_:1}),e("h2",null,[t(l,{bordered:!1,"single-line":!1},{default:a(()=>[fe,pe]),_:1})]),t(i,{style:{color:"#e4572e"}},{default:a(()=>[me]),_:1}),e("h2",null,[t(l,{bordered:!1,"single-line":!1},{default:a(()=>[_e,ye]),_:1})]),t(i,{style:{color:"#e4572e"}},{default:a(()=>[we]),_:1}),t(o,null,{default:a(()=>[be]),_:1}),t(o,null,{default:a(()=>[ve]),_:1})]),_:1})]),_:1})]),_:1})]),_:1})}const Ie=m(A,[["render",ke]]),xe={},Te=s("Motivation"),$e=s(" For my Data Mining class we were tasked with building a Naive Bayes Classifier using the dataset from the "),Be=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://www.kaggle.com/datasets/gaveshjain/ford-sentence-classifiaction-dataset"},"Kaggle hosted by Ford",-1),Ne=s(". I am using the concepts I learned in class and applying them to build a model for this specific dataset. "),Ce=s("Objective"),Ae=s(" The object of this blog is to explain the concepts behind Na\xEFve Bayes, walk through my Na\xEFve Bayes Classifier model, show the results of my model, and walk through some pros and cons of using a Na\xEFve Bayes model. I will also explain any obstacles during this process and how I overcame them as well as any shortcomings in the objective. "),ze=s("Naive Bayes"),Ee=s(" Na\xEFve Bayes is an algorithm that produces probabilities, in my case classes for a dataset, based on the data given. After training a Naive Bayes classifier one simply needs prompt the model with an unknown class that has a set of features. Then it will return the probability of it being a given class. If a binary classification is being used one can assume if the probability of a given class is lower than 50 percent, it is probably the other class. In the case of using multiple classes, you would have to test for each class and choose the highest probability. Na\xEFve Bayes is broken down into two main concepts Bayesian Theorem and Na\xEFve assumption. "),He=s("Bayesian Theorem"),De=s(" Before going into Bayesian Theorem, I would like to explain the difference between a Frequentest and a Bayest. A Frequentest and a Bayest consider the outcomes of past events to make a prediction of possible outcomes of an event. Where these two differ is how they weigh past events that affect their predictions. A Frequentest will take the outcomes of past events and directly applies them to their prediction. In other words, a Frequentest will weigh outcomes of past events heavily and possibly ignores what\u2019s realistic. A Bayest will take the outcomes of past events and apply a small weight on each event that will slightly alter the prediction based on past events. This will be more transparent with an example. "),Fe=s(" Let\u2019s say we have a coin, and we flip it 20 times and it lands on tails 15 times. What\u2019s the probability that the next coin flip will result in tails? Well, if a Frequentest were to guess they would say the probability of tails given a fair coin would be a 75 percent. A Bayest would say the probability of tails given a fair coin would be 50 percent. The Bayest would then look at the outcomes of past events and apply Bayesian Theorem to come up with a probability. "),Me=s(" Bayesian Theorem is the formula that Na\xEFve Bayes uses to produce a probability for a given class based on the data of past events and their outcomes. It uses a Hypothesis (H) and Evidence (E) to made a prediction. Hypothesis is what the algorithm is querying for and Evidence is prior knowledge given. "),je=s("Bayes formula"),Ke=s(" This formula can be broken down into P(H and E)/P(E), where P(H and E) is when both H and E are true. P(H and E) is the joint probability and P(E) is the Margin probability. "),Re=s("Marginalization"),Se=s(" P(E) can be broken down into the summation of the products of the probability of the Hypothesis with the Evidence given the Hypothesis and the opposite of the Hypothesis with Evidence given the opposite of the Hypothesis. "),qe=s("Naive Assumption"),Pe=s(" The Naive part of the Naive Bayes algorithm is the Naive Assumption this algorithm makes about the data given. The algorithm assumes conditional independence between each feature of the dataset. This means each attribute of a given row of data will not affect the outcome of the other features of the same given row of data. This does mean that if the given data's features are dependent on one another the algorithm will fail during the prediction process. "),Ge=s("Source code"),We=s(" In the code below I setup two classes FeedBack and Diction. FeedBack would represent each class and would hold an array of words (Dictions) the FeedBack class contained. I then made each Diction reside in the Feedback array of a given class. Diction would contain the word and the occurrence of that word for the given class. This was so I could make a table like figure making coming up with probabilities easier. "),Oe=s("Document Classes"),Le=s(" After setting up the classes I then took the data from the Kaggle challenge and had it separated into training, development, and testing datasets. They were split 60 percent : 20 percent : 20 percent respectively. I did have to throw away the data that had absent sentences so I could use the data properly. "),Ue=s("Naive Data Setup"),Ve=s(" I setup a Dictionary of all the words utilized in the dataset from the Kaggle challenge. I used the python library 're' to split the sentences by word. I did have to learn Regex from "),Je=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://learn.microsoft.com/en-us/dotnet/standard/base-types/regular-expression-language-quick-reference"},"Microsoft Learn",-1),Qe=s(" so I could gather as many words without special characters as possible while using re.split(). Also I made every word lower case so the I could include words at the start of the sentences. After words any words that were below 5 occurrences were dropped from the dictionary. "),Xe=s("Dictionary Initialization"),Ye=s(" I then used the FeedBack class to get my classes for the dataset. I would store them all in results with each node being a different class it would find when exploring the dataset. If the class already was known it would just add data to the node's array. Additionally, if the word was new to the node's array then I would initialize a new Diction and append it to the end. I also used the 're' python library to split my data and applied the same Regex expression used on the Dictionary. "),Ze=s("Results Initialization"),et=s(' At this point I was ready to try calculating a probability using my formed dataset. I calculated the probability of getting the word "the" out of the entire dataset which just was simply the occurrences of the word "the" divided by the total words in the dataset. The occurrence of "the" was 8095 and the probability of getting "the" was 1.8 percent. '),tt=s('"the" probability'),at=s(' After that I decided to try using Naive Bayes to come up with a probability of getting "the" given that the class was "Requirement". I broke up the math and printed out the result of each operation. Starting with P(Hypothesis), P(Evidence|Hypothesis), P(Evidence|notHypothesis), then P(Evidence) my margin. I then applied the Bayes Theorem formula and came out 3.3 percent chance. This means my chances of getting "the" with the prior knowledge of the word coming from the class "Requirement" was pretty low. '),st=s('"the" given requirement probability '),ot=s(" I then designed a function using the same logic of the code above that could take in any word and class dynamically. This would allow for readability and a nicer look to the function. I could also now call this function multiple times and save time. "),it=s("probs Function"),nt=s(" In the code below I sought out to get the top 10 likely words chosen for each class in my dataset using the training dataset. I would loop through each word in the dictionary and use my probs() function to return the probability of getting that word for the given class. If the word didn't occur in the dataset I then just returned 0.0 probability. The result would then be stored with the word being checked in a node in each classes respective list. After getting all the words and their probabilities for each given class. I sorted each class of words in ascending order and took the first 10 words. "),rt=s("Top 10 setup"),lt=s(` I below is a result of each class's top 10 words and their probabilities given their respective class. This information is holds value as words such as: 'knowledge', 'communication', and 'degree' are present in the results. Which means these words are most likely what recruiters would be looking for on a resume for the given class. However, it is made apparent in the results words such as: "a", "and", "of" are a waste of information as it logically doesn't contribute to the prediction. `),dt=s("Top 10 results"),ct=s(" This is the results of the code I wrote for this challenge. To conclude I wasn't able to get as far as I would like with my code, but I was able to come up with a function that could be used generically to gather probabilities. "),ht=s("Pros & Cons for Naive Bayes"),ut=s("s "),gt=s("Below are the pros and cons of using the Naive Bayes approach which I learned in my classes."),ft=e("thead",null,[e("tr",null,[e("th",null,"Pros"),e("th",null,"Cons")])],-1),pt=e("tbody",null,[e("tr",null,[e("td",null,"The algorithm is relatively simple to implement for a model "),e("td",null,"Fails when features are not independent ")]),e("tr",null,[e("td",null,"Training time is fast O(n) "),e("td",null,"Fails when data is missing (Can be fixed with smoothing or dropping the feature) ")]),e("tr",null,[e("td",null,"Testing time is fast O(1) "),e("td")])],-1),mt=s("Contributions"),_t=e("thead",null,[e("tr",null,[e("th",null,"Contribution"),e("th",null,"The value of the contribution"),e("th",null,"Technical Challenges")])],-1),yt=e("tbody",null,[e("tr",null,[e("td",null,"Used regex to take off unneeded special characters"),e("td",null,"Allowed the data to be used for raw words that didn't have any extra unneeded information"),e("td",null,"Had to allow sum words to be missed since the the special characters such as single quotes needed to remain. I also had to learn regex and how to utilize it for the splitting. ")]),e("tr",null,[e("td",null,"Made a dictionary and class table of the given words"),e("td",null,"I made a dictionary of all the words used with their occurrences and then made an array that acted as a class table for the class, the word, and their occurrences given the class. "),e("td",null,"Had to take words off that occurred less than 5 times. ")]),e("tr",null,[e("td",null,'Made a probability function "probs()"'),e("td",null,"I made a dynamic probability function that given the word and the class would output a probability of the word given the class using the Naive Bayes formula. "),e("td",null,"Only could make probabilities of words given classes, but not classes given words. ")]),e("tr",null,[e("td",null,"Explained Naive Bayes and core principles of the math behind it"),e("td",null,"I gave an explanation of what Naive Bayes is and gave an explanation to core principles such as: Bayes vs. Frequentest, Bayes Theorem, and Naive Assumption. "),e("td",null,"No issues. ")]),e("tr",null,[e("td",null,"Made illustrations of the math myself"),e("td",null,"All images and explanations used were made by myself using what I learned in class. "),e("td",null,"No issues. ")])],-1),wt=s("References"),bt=e("thead",null,[e("tr",null,[e("th",null,"Reference"),e("th",null,"How I used the reference"),e("th",null,"What value I made over the reference")])],-1),vt=e("tbody",null,[e("tr",null,[e("td",null,'"https://learn.microsoft.com/en-us/dotnet/standard/base-types/regular-expression-language-quick-reference" '),e("td",null,"I used this website to learn Regex "),e("td",null,'This allowed me to split my sentences using spaces and other delimiters for the "re" python library. ')])],-1),kt=s("Repository:"),It=e("a",{style:{color:"#e07a5f"},href:"https://github.com/ChristopherGordSmith/NaiveBayesClassifier",target:"_blank"}," This Blogs Repository",-1),xt=s("Downloadable Code:"),Tt=e("a",{style:{color:"#e07a5f"},href:"assets/Data-Mining-Assignment-2.ipynb",download:""}," My Naive Bayes Model",-1);function $t(g,f){const i=r("n-h1"),o=r("n-h2"),n=r("n-image"),l=r("n-h5"),d=r("n-table"),c=r("n-space"),p=r("n-gi"),_=r("n-grid");return h(),u(c,{vertical:""},{default:a(()=>[t(_,{cols:"1",responsive:"screen","x-gap":100},{default:a(()=>[t(p,null,{default:a(()=>[t(c,null,{default:a(()=>[t(i,{style:{color:"#e4572e"}},{default:a(()=>[Te]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[$e,Be,Ne]),_:1}),t(i,{style:{color:"#e4572e"}},{default:a(()=>[Ce]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Ae]),_:1}),t(i,{style:{color:"#e4572e"}},{default:a(()=>[ze]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Ee]),_:1}),t(i,{style:{color:"#e4572e"}},{default:a(()=>[He]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[De]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Fe]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Me,t(n,{style:{"margin-bottom":"0px","margin-left":"30%","margin-right":"30%"},src:"assets/BayesTheorem.jpg",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[je]),_:1}),Ke,t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/BayesTheoremEvidence.jpg",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[Re]),_:1}),Se]),_:1}),t(i,{style:{color:"#e4572e"}},{default:a(()=>[qe]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Pe]),_:1}),t(i,{style:{color:"#e4572e"}},{default:a(()=>[Ge]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[We,t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/NaiveClasses.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[Oe]),_:1}),Le,t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/NaiveDataSetup.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[Ue]),_:1}),Ve,Je,Qe,t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/DictionaryInitialize.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[Xe]),_:1}),Ye,t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/ResultsInitialize.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[Ze]),_:1}),et,t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/theProbs.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[tt]),_:1}),at,t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/theGivenReq.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[st]),_:1}),ot,t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/probsFunction.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[it]),_:1}),nt,t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/MakingTop10.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[rt]),_:1}),lt,t(n,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/top10words.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[dt]),_:1}),ct]),_:1}),t(i,{style:{color:"#e4572e"}},{default:a(()=>[ht]),_:1}),ut,t(o,{style:{"font-weight":"700"}},{default:a(()=>[t(o,{style:{color:"white"}},{default:a(()=>[gt]),_:1}),t(d,{bordered:!1,"single-line":!1,style:{"margin-left":"20%","margin-right":"20%"}},{default:a(()=>[ft,pt]),_:1})]),_:1}),e("h2",null,[t(i,{style:{color:"#e4572e"}},{default:a(()=>[mt]),_:1}),t(d,{bordered:!1,"single-line":!1},{default:a(()=>[_t,yt]),_:1})]),t(i,{style:{color:"#e4572e"}},{default:a(()=>[wt]),_:1}),e("h2",null,[t(d,{bordered:!1,"single-line":!1},{default:a(()=>[bt,vt]),_:1})]),t(i,{style:{color:"#e4572e"}},{default:a(()=>[kt]),_:1}),t(o,null,{default:a(()=>[It]),_:1}),t(i,{style:{color:"#e4572e"}},{default:a(()=>[xt]),_:1}),t(o,null,{default:a(()=>[Tt]),_:1})]),_:1})]),_:1})]),_:1})]),_:1})}const Bt=m(xe,[["render",$t]]),Nt=s("Kaggle Titanic Tutorial"),Ct=s("Image Classifier"),At=s("Naive Bayes Classifier"),Et=y({__name:"Blogs",setup(g){return(f,i)=>{const o=r("n-h1"),n=r("n-collapse-item"),l=r("n-collapse"),d=r("n-space");return h(),u(d,{style:{padding:"50px 100px 50px 100px"}},{default:a(()=>[t(l,{"default-expanded-names":"1",accordion:""},{default:a(()=>[t(n,null,{header:a(()=>[t(o,{style:{color:"#e4572e","font-size":"40px"}},{default:a(()=>[Nt]),_:1})]),default:a(()=>[t(C)]),_:1}),t(n,null,{header:a(()=>[t(o,{style:{color:"#e4572e","font-size":"40px"}},{default:a(()=>[Ct]),_:1})]),default:a(()=>[t(Ie)]),_:1}),t(n,null,{header:a(()=>[t(o,{style:{color:"#e4572e","font-size":"40px"}},{default:a(()=>[At]),_:1})]),default:a(()=>[t(Bt)]),_:1})]),_:1})]),_:1})}}});export{Et as default};
