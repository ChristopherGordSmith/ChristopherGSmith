import{_ as p,r,o as u,c as g,w as a,a as t,b as s,d as e,e as y}from"./index.395ddb66.js";const w={},b=s(" I followed through a tutorial by Alexis Cook that went through the process of completing in a Kaggle competition. The practice competition uses a passenger dataset from the Titanic. The dataset specifies a passengers attributes such as age, sex, ticket class, etc. The dataset also indicates if they survived or not. The challenge is to create a prediction model that predicts whether a given passenger would survive. "),v=s("Going into the directory with the training data, testing data and example prediction .csv files."),I=s("Assigning training data and testing data. Also displaying the 1st five passengers with their respective data for each dataset."),k=s("Testing if gender_submission.csv prediction of all female passengers survived and all male passengers died. Since the numbers are 74% women and 18% men we can say that this prediction is significantly inaccurate. "),T=s("Using the Random Tree Model to make predictions based on a given passenger's data."),x=s(" After following the tutorial from Alexis Cook we built a prediction model that uses a random forest model. A random forest model produces trees that take each attribute of the passenger's data into account. The model uses a majority vote with the outcome of the trees to predict if the passenger survived. The model we built only accounted for the passenger's class, sex, amount of siblings and spouses, amount of parents and children. This model produced a score of .77511 accuracy. "),N=e("h1",{style:{color:"white"}},"Reference:",-1),$=e("a",{href:"https://www.kaggle.com/code/alexisbcook/titanic-tutorial/notebook"},"Alexis Cook's Titanic Kaggle Tutorial (Click me)",-1);function C(m,f){const n=r("n-h2"),o=r("n-image"),i=r("n-space"),l=r("n-gi"),h=r("n-grid");return u(),g(i,{vertical:""},{default:a(()=>[t(h,{cols:"1",responsive:"screen","x-gap":100},{default:a(()=>[t(l,null,{default:a(()=>[t(i,null,{default:a(()=>[t(n,{style:{color:"white"}},{default:a(()=>[b]),_:1}),t(n,{style:{color:"white"}},{default:a(()=>[v]),_:1}),t(o,{src:"assets/KaggleTutorialSnip1.png",width:"800"}),t(n,{style:{color:"white"}},{default:a(()=>[I]),_:1}),t(o,{src:"assets/KaggleTutorialSnip2.png",width:"800"}),t(n,{style:{color:"white"}},{default:a(()=>[k]),_:1}),t(o,{src:"assets/KaggleTutorialSnip3.png",width:"800"}),t(n,{style:{color:"white"}},{default:a(()=>[T]),_:1}),t(o,{src:"assets/KaggleTutorialSnip4.png",width:"800"}),t(n,{style:{color:"white"}},{default:a(()=>[x]),_:1}),t(o,{src:"assets/KaggleTutorialSnip5.png",width:"800"})]),_:1})]),_:1})]),_:1}),N,$]),_:1})}const B=p(w,[["render",C]]),K={},A=s("Motivation"),j=s(" For my Data Mining class we were assigned to use a dataset from "),S=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://www.kaggle.com/datasets/maricinnamon/caltech101-airplanes-motorbikes-schooners"},"caltech101 airplanes-motorbikes-schooners competition",-1),M=s(" . The objective was to derive a model to predict the classes of each image and through experimentation increase the performance of the model. This is my attempt to increase the performance of the model. "),z=s("Source code and background"),F=s(" The dataset used is from "),D=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://www.kaggle.com/datasets/maricinnamon/caltech101-airplanes-motorbikes-schooners"},"caltech101 airplanes-motorbikes-schooners competition",-1),E=s(" . It contains 3 folders of airplanes, motorbikes and schooners. To start the assignment, I looked online for source code that implemented Convolution Neural Networks for the Machine Learning process while being able to process the images from the dataset. That\u2019s when I stumbled upon Geeks for Geeks "),H=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://www.geeksforgeeks.org/python-image-classification-using-keras/"},"\u201CPython Image Classification using Keras\u201D",-1),V=s(" article. In the article they use the Keras library for processing the images, creating, and training a model, and predicting the images. However, the approach they used was an 80:20 split for their training and testing. Additionally, it was only concerned with classification for 2 classes."),P=s(" I decided to implement a part of the source code from this article and would look for source code that implemented prediction and training for more than 2 classes. I found a medium article "),R=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720"},"\u201CTutorial on using Keras flow_from_directory and generators\u201D",-1),q=s(" , that gave a tutorial for the training and prediction process. This article also only classified 2 classes for cats and dogs, but implemented a training, validation, and test datasets using Keras model again. "),W=s(" In both models they used \u201Cbinary cross entropy\u201D for loss, \u201Crmsprop\u201D for optimization, and their metrics were \u201Caccuracy\u201D. Binary cross entropy\u2019s loss function is great for binary classification and a great article I found is medium\u2019s "),G=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a"},"\u201CUnderstanding binary cross-entropy / logs loss: a visual explanation\u201D",-1),L=s(" . To give a brief explanation what loss functions do in convolutional neural networks. The output is evaluated at the end of forward propagation to evaluate how far off the model is from the desired goal/output of the given input by using a given loss function. In the case of using multiple classes binary cross entropy isn\u2019t best suited. "),O=s(" Therefore, in my model I used categorical cross entropy as the loss function. This loss function is ideal as it takes the probabilities of each class and adjusts the weights in favor of the actual class. As opposed to binary cross entropy which is concerned about the probability of the class being a class or not. This means it isn\u2019t really concerned about identifying other images if it can classify one and say it is or is not that class. "),U=s(" They used \u201Crmsprop\u201D for optimization and after reading over an article from TowardsDataScience, "),Y=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b"},"\u201CA look at gradient descent and rmsprop optimizers\u201D",-1),J=s(" it seems that they used this in order to increase the learning rate with larger steps without worrying about overstepping during the descent. For my model I decided to use \u201Csgd\u201D or Stochastic Gradient Descent. I read an article by geeks for geeks "),Q=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/"},"SGD",-1),X=s(" on how the optimizer works. I simply used this to try and derive different results that could be compared to using rmsprop. "),Z=s(' Lastly, their model used simply \u201Caccuracy\u201D as their metric, which gives the difference between the prediction and the actual. However, in the case of multi-classification, we need a metric that gives us a sum of the correctly matched/total images. This is more realistic since we aren\u2019t measuring the probability of it being a class, but if the class matched was correct or not. Therefore, I used "categorical accuracy" as my metric for the model. '),ee=s("What is forward and backward propagation?"),te=s(" Forward propagation is passing input data through layers that reach an output layer. The input goes through layers known as \u201CNode layers\u201D, and they have set weights on each node that performs an operation on the input. These layers are the hidden layers of a neural network and for this network to exist it must possess at least one hidden layer. Once the input reaches the output layer the output is then evaluated based on how far off the desired output was. After the output is assessed, backward propagation begins. Backward propagation goes backward going through each layer and adjusting each node of each layer in hopes to close the gap between the output and the desired output or decrease the \u201Closs\u201D. "),ae=s("Experiments & results"),se=s(" In the experiments I would adjust the hyper parameters of the model to derive different results. Each result was recording on an "),oe=e("a",{style:{color:"#e07a5f"},href:"assets/Assignment_1_Experiment_Data.xlsx",download:""},"excel sheet",-1),ie=s(" with each row representing an experiment. The columns representing the results: training time (seconds), accuracy, batch size, epochs, node layers, loss(percent). "),ne=s(" This is the result of the initial state of the program, after applying changes to make it work with the new dataset and a 60:20:20 ratio for training, validation, and testing. Through out the experiments I would monitor the training vs. validation performance to make sure that over fitting wouldn't occur and if so I would report it in the experiment. "),re=s(" For accuracy I would use this prediction output for my accuracy that I calculated myself. Note that the above image does have an accuracy for training and validation, but I used the prediction model output as it was tested on the test dataset. Also my accuracy throughout these experiments never changed as I was unsuccessful in even changing the result of the accuracy. "),le=s(" From the initial state to the 1st experiment I added an extra layer to the training model to see if this could possibly improve performance. This did not improve performance and actually resulted in a negative effect in loss and accuracy. "),he=s(" In this experiment I kept the initial state and doubled the epochs from 10 to 20 in hopes that more runs in the training model would result in increased performance. This actually did increase the accuracy, but at the cost of increased training time. However, according to the line graph my training had over fitting occur. "),de=s(" In this experiment I kept the initial state and doubled the batch size from 16 to 32. The result of this was a slightly faster training time. The training model did take a slight decrease in accuracy and a significant increase in loss. "),ce=s("Conclusion"),ue=s(" To conclude this blog I found that increasing the epochs did increase the accuracy of the model. However, it did result in over fitting as the training performance exceeded the validation performance. Increasing the layers did not result in better performance and actually increased loss and accuracy significantly. Doubling the batch size also did not yield any not worth results. Lastly, the initial state of the model from the geeks for geeks tutorial also suffered from over performance. This means experiment #1's result yielded the best outcome. "),ge=s("Contributions"),me=e("thead",null,[e("tr",null,[e("th",null,"Contribution"),e("th",null,"The value of the contribution"),e("th",null,"Technical Challenges")])],-1),fe=e("tbody",null,[e("tr",null,[e("td",null,"Explanation of forward and backward propagation"),e("td",null,"I explained at a high level the process of forward and backward propagation in my own words."),e("td",null,"No technical difficulties")]),e("tr",null,[e("td",null,"Model metric"),e("td",null,"I added into the model a different metric rather than using accuracy I used categorical accuracy."),e("td",null," No technical difficulties ")]),e("tr",null,[e("td",null,"Experiment #1"),e("td",null," In this experiment I changed the hyperparameter of neural layers from 3 to 4 and had the output filters increase and then decrease in the layers. The outcome of this experiment resulted in a decrease in accuracy and increase in loss. Showing that I was possibly filtering the inputs too much to the point that nothing accurate could be derived from it. "),e("td",null,"No technical difficulties")]),e("tr",null,[e("td",null,"Experiment #2"),e("td",null," In this experiment I changed the hyperparameter of epochs from 10 to 20. The outcome of this experiment resulted in a increase in accuracy and decrease in loss. Show that increasing the model epochs correlate to an increased performance, but longer training time. "),e("td",null,"No technical difficulties")]),e("tr",null,[e("td",null,"Experiment #3"),e("td",null," In this experiment I changed the hyperparameter of batch sizes from 16 to 32. The outcome of this experiment resulted in a decrease in accuracy and increase in loss. A positive of this is the training time did decrease. This shows that the batch size increase in this scenario had little to no difference on the model. "),e("td",null,"No technical difficulties")]),e("tr",null,[e("td",null,"Converted the binary classification model to a multi-classification model"),e("td",null,"I was able to convert the training model from the geeks for geeks tutorial to a multi-classification model."),e("td",null,' I spent 3 hours trying to figure out how to solve an error I would get when using "categorical cross-entropy". Turns out the model was pushing out only 1 node and not the same amount of nodes as there were classes in the training model. It also explained why my accuracy was also very consistent. ')])],-1),pe=s("Reference"),_e=e("thead",null,[e("tr",null,[e("th",null,"Reference"),e("th",null,"How I used the reference"),e("th",null,"What value I made over the reference")])],-1),ye=e("tbody",null,[e("tr",null,[e("td",null,'"https://www.kaggle.com/datasets/maricinnamon/caltech101-airplanes-motorbikes-schooners"'),e("td",null,"I used their dataset for my experiments"),e("td",null," I converted each of these folders into a training, validation, and test folder. I used a 60:20:20 ratio respectively. Each folder also needed to have a subfolder for motorbikes, airplanes, and schooners that contained their respective images. This was done so the Keras model would work with this dataset. This took me an hour to implement. ")]),e("tr",null,[e("td",null,'"https://www.geeksforgeeks.org/python-image-classification-using-keras/"'),e("td",null,"I used their training model in my code, but not their data generator and prediction code."),e("td",null,"This allowed me to experiment with the hyper parameters of the data.")]),e("tr",null,[e("td",null,'"https://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720" '),e("td",null,"I used their data generator and prediction code."),e("td",null,"This allowed me to derive results from the experiments I conducted on the model using various hyper parameters.")]),e("tr",null,[e("td",null,'"https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a" '),e("td",null,"I read this article to give me an understanding of the binary cross entropy loss function."),e("td",null,"I used what I learned from this article to explain in my blog why using this loss function wouldn't work for multi-classification. ")]),e("tr",null,[e("td",null,'"https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b" '),e("td",null,"I read this article to give me an understanding of rmsprop optimizers."),e("td",null,"I got an understanding that the reason the code I referenced used and rmsprop optimizer was to allow for greater learning rates. This optimizer allows for greater learning rates without the fear of over stepping on the descent. ")]),e("tr",null,[e("td",null,'"https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/" '),e("td",null,"I read this article to give me an understanding of stochastic gradient descent."),e("td",null,"I was able to derive that rmsprop optimizer and sgd optimizer are very similar, but rmsprop allows for a faster learning rate. I would like to experiment with these different optimizer to see if I get different results. ")])],-1),we=s("Downloadable Code:"),be=e("a",{style:{color:"#e07a5f"},href:"assets/Reference_Code.ipynb",download:""},"Original code from geeks for geeks",-1),ve=e("a",{style:{color:"#e07a5f"},href:"assets/data-mining-assignment-1.ipynb",download:""},"My implementation with experiments",-1);function Ie(m,f){const n=r("n-h1"),o=r("n-h2"),i=r("n-image"),l=r("n-table"),h=r("n-space"),d=r("n-gi"),c=r("n-grid");return u(),g(h,{vertical:""},{default:a(()=>[t(c,{cols:"1",responsive:"screen","x-gap":100},{default:a(()=>[t(d,null,{default:a(()=>[t(h,null,{default:a(()=>[t(n,{style:{color:"#e4572e"}},{default:a(()=>[A]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[j,S,M]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[z]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[F,D,E,H,V]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[P,R,q]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[W,G,L]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[O]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[U,Y,J,Q,X]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Z]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[ee]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[te]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[ae]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[se,oe,ie]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[ne]),_:1}),t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/initial_state_loss_graph.png",width:"800"}),t(o,{style:{color:"white"}},{default:a(()=>[re]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[le]),_:1}),t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/Extra_layer.png",width:"800"}),t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/experiment_1_loss_graph.png",width:"800"}),t(o,{style:{color:"white"}},{default:a(()=>[he]),_:1}),t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/double_epochs.png",width:"800"}),t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/experiment_2_loss_graph.png",width:"800"}),t(o,{style:{color:"white"}},{default:a(()=>[de]),_:1}),t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/double_batch.png",width:"800"}),t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/experiment_3_loss_graph.png",width:"800"}),t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/all_experiments.png",width:"800"}),t(o,{style:{color:"white"}},{default:a(()=>[t(n,{style:{color:"#e4572e"}},{default:a(()=>[ce]),_:1}),ue]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[ge]),_:1}),e("h2",null,[t(l,{bordered:!1,"single-line":!1},{default:a(()=>[me,fe]),_:1})]),t(n,{style:{color:"#e4572e"}},{default:a(()=>[pe]),_:1}),e("h2",null,[t(l,{bordered:!1,"single-line":!1},{default:a(()=>[_e,ye]),_:1})]),t(n,{style:{color:"#e4572e"}},{default:a(()=>[we]),_:1}),t(o,null,{default:a(()=>[be]),_:1}),t(o,null,{default:a(()=>[ve]),_:1})]),_:1})]),_:1})]),_:1})]),_:1})}const ke=p(K,[["render",Ie]]),Te={},xe=s("Motivation"),Ne=s(" For my Data Mining class we were tasked with building a Naive Bayes Classifier using the dataset from the "),$e=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://www.kaggle.com/datasets/gaveshjain/ford-sentence-classifiaction-dataset"},"Kaggle hosted by Ford",-1),Ce=s(". I am using the concepts I learned in class and applying them to build a model for this specific dataset. "),Be=s("Objective"),Ke=s(" The object of this blog is to explain the concepts behind Na\xEFve Bayes, walk through my Na\xEFve Bayes Classifier model, show the results of my model, and walk through some pros and cons of using a Na\xEFve Bayes model. I will also explain any obstacles during this process and how I overcame them as well as any shortcomings in the objective. "),Ae=s("Naive Bayes"),je=s(" Na\xEFve Bayes is an algorithm that produces probabilities, in my case classes for a dataset, based on the data given. After training a Naive Bayes classifier one simply needs prompt the model with an unknown class that has a set of features. Then it will return the probability of it being a given class. If a binary classification is being used one can assume if the probability of a given class is lower than 50 percent, it is probably the other class. In the case of using multiple classes, you would have to test for each class and choose the highest probability. Na\xEFve Bayes is broken down into two main concepts Bayesian Theorem and Na\xEFve assumption. "),Se=s("Bayesian Theorem"),Me=s(" Before going into Bayesian Theorem, I would like to explain the difference between a Frequentest and a Bayest. A Frequentest and a Bayest consider the outcomes of past events to make a prediction of possible outcomes of an event. Where these two differ is how they weigh past events that affect their predictions. A Frequentest will take the outcomes of past events and directly applies them to their prediction. In other words, a Frequentest will weigh outcomes of past events heavily and possibly ignores what\u2019s realistic. A Bayest will take the outcomes of past events and apply a small weight on each event that will slightly alter the prediction based on past events. This will be more transparent with an example. "),ze=s(" Let\u2019s say we have a coin, and we flip it 20 times and it lands on tails 15 times. What\u2019s the probability that the next coin flip will result in tails? Well, if a Frequentest were to guess they would say the probability of tails given a fair coin would be a 75 percent. A Bayest would say the probability of tails given a fair coin would be 50 percent. The Bayest would then look at the outcomes of past events and apply Bayesian Theorem to come up with a probability. "),Fe=s(" Bayesian Theorem is the formula that Na\xEFve Bayes uses to produce a probability for a given class based on the data of past events and their outcomes. It uses a Hypothesis (H) and Evidence (E) to made a prediction. Hypothesis is what the algorithm is querying for and Evidence is prior knowledge given. "),De=s("Bayes formula"),Ee=s(" This formula can be broken down into P(H and E)/P(E), where P(H and E) is when both H and E are true. P(H and E) is the joint probability and P(E) is the Margin probability. "),He=s("Marginalization"),Ve=s(" P(E) can be broken down into the summation of the products of the probability of the Hypothesis with the Evidence given the Hypothesis and the opposite of the Hypothesis with Evidence given the opposite of the Hypothesis. "),Pe=s("Naive Assumption"),Re=s(" The Naive part of the Naive Bayes algorithm is the Naive Assumption this algorithm makes about the data given. The algorithm assumes conditional independence between each feature of the dataset. This means each attribute of a given row of data will not affect the outcome of the other features of the same given row of data. This does mean that if the given data's features are dependent on one another the algorithm will fail during the prediction process. "),qe=s("Source code"),We=s(" In the code below I setup two classes FeedBack and Diction. FeedBack would represent each class and would hold an array of words (Dictions) the FeedBack class contained. I then made each Diction reside in the Feedback array of a given class. Diction would contain the word and the occurrence of that word for the given class. This was so I could make a table like figure making coming up with probabilities easier. "),Ge=s("Document Classes"),Le=s(" After setting up the classes I then took the data from the Kaggle challenge and had it separated into training, development, and testing datasets. They were split 60 percent : 20 percent : 20 percent respectively. I did have to throw away the data that had absent sentences so I could use the data properly. "),Oe=s("Naive Data Setup"),Ue=s(" I setup a Dictionary of all the words utilized in the dataset from the Kaggle challenge. I used the python library 're' to split the sentences by word. I did have to learn Regex from "),Ye=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://learn.microsoft.com/en-us/dotnet/standard/base-types/regular-expression-language-quick-reference"},"Microsoft Learn",-1),Je=s(" so I could gather as many words without special characters as possible while using re.split(). Also I made every word lower case so the I could include words at the start of the sentences. After words any words that were below 5 occurrences were dropped from the dictionary. "),Qe=s("Dictionary Initialization"),Xe=s(" I then used the FeedBack class to get my classes for the dataset. I would store them all in results with each node being a different class it would find when exploring the dataset. If the class already was known it would just add data to the node's array. Additionally, if the word was new to the node's array then I would initialize a new Diction and append it to the end. I also used the 're' python library to split my data and applied the same Regex expression used on the Dictionary. "),Ze=s("Results Initialization"),et=s(' At this point I was ready to try calculating a probability using my formed dataset. I calculated the probability of getting the word "the" out of the entire dataset which just was simply the occurrences of the word "the" divided by the total words in the dataset. The occurrence of "the" was 8095 and the probability of getting "the" was 1.8 percent. '),tt=s('"the" probability'),at=s(' After that I decided to try using Naive Bayes to come up with a probability of getting "the" given that the class was "Requirement". I broke up the math and printed out the result of each operation. Starting with P(Hypothesis), P(Evidence|Hypothesis), P(Evidence|notHypothesis), then P(Evidence) my margin. I then applied the Bayes Theorem formula and came out 3.3 percent chance. This means my chances of getting "the" with the prior knowledge of the word coming from the class "Requirement" was pretty low. '),st=s('"the" given requirement probability '),ot=s(" I then designed a function using the same logic of the code above that could take in any word and class dynamically. This would allow for readability and a nicer look to the function. I could also now call this function multiple times and save time. "),it=s("probs Function"),nt=s(" In the code below I sought out to get the top 10 likely words chosen for each class in my dataset using the training dataset. I would loop through each word in the dictionary and use my probs() function to return the probability of getting that word for the given class. If the word didn't occur in the dataset I then just returned 0.0 probability. The result would then be stored with the word being checked in a node in each classes respective list. After getting all the words and their probabilities for each given class. I sorted each class of words in ascending order and took the first 10 words. "),rt=s("Top 10 setup"),lt=s(` I below is a result of each class's top 10 words and their probabilities given their respective class. This information is holds value as words such as: 'knowledge', 'communication', and 'degree' are present in the results. Which means these words are most likely what recruiters would be looking for on a resume for the given class. However, it is made apparent in the results words such as: "a", "and", "of" are a waste of information as it logically doesn't contribute to the prediction. `),ht=s("Top 10 results"),dt=s(" This is the results of the code I wrote for this challenge. To conclude I wasn't able to get as far as I would like with my code, but I was able to come up with a function that could be used generically to gather probabilities. "),ct=s("Pros & Cons for Naive Bayes"),ut=s("s "),gt=s("Below are the pros and cons of using the Naive Bayes approach which I learned in my classes."),mt=e("thead",null,[e("tr",null,[e("th",null,"Pros"),e("th",null,"Cons")])],-1),ft=e("tbody",null,[e("tr",null,[e("td",null,"The algorithm is relatively simple to implement for a model "),e("td",null,"Fails when features are not independent ")]),e("tr",null,[e("td",null,"Training time is fast O(n) "),e("td",null,"Fails when data is missing (Can be fixed with smoothing or dropping the feature) ")]),e("tr",null,[e("td",null,"Testing time is fast O(1) "),e("td")])],-1),pt=s("Contributions"),_t=e("thead",null,[e("tr",null,[e("th",null,"Contribution"),e("th",null,"The value of the contribution"),e("th",null,"Technical Challenges")])],-1),yt=e("tbody",null,[e("tr",null,[e("td",null,"Used regex to take off unneeded special characters"),e("td",null,"Allowed the data to be used for raw words that didn't have any extra unneeded information"),e("td",null,"Had to allow sum words to be missed since the the special characters such as single quotes needed to remain. I also had to learn regex and how to utilize it for the splitting. ")]),e("tr",null,[e("td",null,"Made a dictionary and class table of the given words"),e("td",null,"I made a dictionary of all the words used with their occurrences and then made an array that acted as a class table for the class, the word, and their occurrences given the class. "),e("td",null,"Had to take words off that occurred less than 5 times. ")]),e("tr",null,[e("td",null,'Made a probability function "probs()"'),e("td",null,"I made a dynamic probability function that given the word and the class would output a probability of the word given the class using the Naive Bayes formula. "),e("td",null,"Only could make probabilities of words given classes, but not classes given words. ")]),e("tr",null,[e("td",null,"Explained Naive Bayes and core principles of the math behind it"),e("td",null,"I gave an explanation of what Naive Bayes is and gave an explanation to core principles such as: Bayes vs. Frequentest, Bayes Theorem, and Naive Assumption. "),e("td",null,"No issues. ")]),e("tr",null,[e("td",null,"Made illustrations of the math myself"),e("td",null,"All images and explanations used were made by myself using what I learned in class. "),e("td",null,"No issues. ")])],-1),wt=s("References"),bt=e("thead",null,[e("tr",null,[e("th",null,"Reference"),e("th",null,"How I used the reference"),e("th",null,"What value I made over the reference")])],-1),vt=e("tbody",null,[e("tr",null,[e("td",null,'"https://learn.microsoft.com/en-us/dotnet/standard/base-types/regular-expression-language-quick-reference" '),e("td",null,"I used this website to learn Regex "),e("td",null,'This allowed me to split my sentences using spaces and other delimiters for the "re" python library. ')])],-1),It=s("Repository:"),kt=e("a",{style:{color:"#e07a5f"},href:"https://github.com/ChristopherGordSmith/NaiveBayesClassifier",target:"_blank"}," This Blogs Repository",-1),Tt=s("Downloadable Code:"),xt=e("a",{style:{color:"#e07a5f"},href:"assets/Data-Mining-Assignment-2.ipynb",download:""}," My Naive Bayes Model",-1);function Nt(m,f){const n=r("n-h1"),o=r("n-h2"),i=r("n-image"),l=r("n-h5"),h=r("n-table"),d=r("n-space"),c=r("n-gi"),_=r("n-grid");return u(),g(d,{vertical:""},{default:a(()=>[t(_,{cols:"1",responsive:"screen","x-gap":100},{default:a(()=>[t(c,null,{default:a(()=>[t(d,null,{default:a(()=>[t(n,{style:{color:"#e4572e"}},{default:a(()=>[xe]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Ne,$e,Ce]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[Be]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Ke]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[Ae]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[je]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[Se]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Me]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[ze]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Fe,t(i,{style:{"margin-bottom":"0px","margin-left":"30%","margin-right":"30%"},src:"assets/BayesTheorem.jpg",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[De]),_:1}),Ee,t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/BayesTheoremEvidence.jpg",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[He]),_:1}),Ve]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[Pe]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Re]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[qe]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[We,t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/NaiveClasses.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[Ge]),_:1}),Le,t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/NaiveDataSetup.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[Oe]),_:1}),Ue,Ye,Je,t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/DictionaryInitialize.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[Qe]),_:1}),Xe,t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/ResultsInitialize.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[Ze]),_:1}),et,t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/theProbs.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[tt]),_:1}),at,t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/theGivenReq.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[st]),_:1}),ot,t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/probsFunction.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[it]),_:1}),nt,t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/MakingTop10.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[rt]),_:1}),lt,t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/top10words.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"45%","margin-left":"45%"}},{default:a(()=>[ht]),_:1}),dt]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[ct]),_:1}),ut,t(o,{style:{"font-weight":"700"}},{default:a(()=>[t(o,{style:{color:"white"}},{default:a(()=>[gt]),_:1}),t(h,{bordered:!1,"single-line":!1,style:{"margin-left":"20%","margin-right":"20%"}},{default:a(()=>[mt,ft]),_:1})]),_:1}),e("h2",null,[t(n,{style:{color:"#e4572e"}},{default:a(()=>[pt]),_:1}),t(h,{bordered:!1,"single-line":!1},{default:a(()=>[_t,yt]),_:1})]),t(n,{style:{color:"#e4572e"}},{default:a(()=>[wt]),_:1}),e("h2",null,[t(h,{bordered:!1,"single-line":!1},{default:a(()=>[bt,vt]),_:1})]),t(n,{style:{color:"#e4572e"}},{default:a(()=>[It]),_:1}),t(o,null,{default:a(()=>[kt]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[Tt]),_:1}),t(o,null,{default:a(()=>[xt]),_:1})]),_:1})]),_:1})]),_:1})]),_:1})}const $t=p(Te,[["render",Nt]]),Ct={},Bt=s("Motivation"),Kt=s(" For my Data Mining class we were tasked with comparing classification algorithms. The task was to be carried out by implementing the code for these algorithms and test them against our choice of a "),At=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"httpshttps://www.kaggle.com/datasets/vivekgediya/ecommerce-product-review-data"},"text classifier dataset",-1),jt=s(" or an "),St=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://www.kaggle.com/datasets/whenamancodes/wild-animals-images"},"image classifier dataset",-1),Mt=s(". I chose the latter and decided to compare the algorithms Convolutional Neural Networks, Support Vector Machines, and K Nearest Neighbors. "),zt=s("Objective"),Ft=s(" The object of this blog is to do a comparison of each algorithm and show their performances on the "),Dt=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://www.kaggle.com/datasets/whenamancodes/wild-animals-images"},"wild animals dataset",-1),Et=s(". I'll also give a walk through of each algorithm I'm comparing to give you an understanding of each one. I will also explain any obstacles during this process and how I overcame them as well as any shortcomings in the objective. "),Ht=s("Convolutional Neural Networks"),Vt=s(` I have a blog post already going over Convolutional Neural Networks above under the "Image Classifier" post. I'll be going over it again, but in less detail. Convolutional Neural Networks are algorithm with layers of nodes that a receive inputs and slightly adjust the given inputs for a desired output. A Neural Networks must have at least one hidden layer that separates the input layer from the output layer. This layer performs an adjustment on the inputs to an extent. This extent is a weight that depending on the weight will dramatically alter it or slightly alter it. These networks learn by measuring the difference in the result and the desired output and have the weights readjusted accordingly. I go over the concept of Forward Propagation and Backward Propagation in my blog post "Image Classifier". The amount of outputs of this network can vary, but for my purposes the outputs were 6 with each output node representing the probability of the image belonging to a given class. `),Pt=s("Support Vector Machines"),Rt=s(" Support Vector Machines use matrices to map out objects based on their features. The dimensions of the matrix is based on the amount of features the object has. In our case the dimensions are based on how many pixels the image has. This is because the code I used was specifically looking for the intensity at each pixel. After plotting each object or in our case each image. It then measures each image from one to another based on their features and class. With these measurements it attempts to create a line that will segregate the classes from one another. This line will attempt to form itself to give each class as great of a margin of error as it can. This is an attempt to give the classes breathing room to be slightly different from one another so when predicting a class we have a better chance at being correct. This line is called a support vector which is where this algorithm gets its name from. "),qt=s("K Nearest Neighbors"),Wt=s(" K Nearest Neighbors also uses matrices to map out object based on their features. The dimensions are also based on the amount of features the object has. This algorithm will also use each image's pixel intensities to determine uniqueness. However, where Support Vector Machines and K Nearest Neighbors differs is how they train and predict. K Nearest Neighbors doesn't have a training time as it doesn't need to create a vector/line to separate each class. Instead when asked to predict the class of an object/image they simply grab their K Nearest Neighbors. They measure each feature from one another using Euclidean Distance. "),Gt=s("Euclidean Distance "),Lt=s(" Where x and y are two objects and i is a feature of the objects. Once comparing the distance of each object to the object to be predicted it then grabs its nearest K neighbors. This is where K Nearest Neighbors gets its name. K is a hyper parameter that must be adjusted to give better results. You have to make sure the K is not grabbing too many neighbors and not too little. After grabbing the neighbors the majority class wins the prediction. Normally you want your k to be an odd number to prevent ties. "),Ot=s("Source code for Convolutional Neural Networks"),Ut=s(' The code for this blog is the same as the code used in the "Image Classifier" blog post. The code came from two tutorials, '),Yt=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://www.geeksforgeeks.org/python-image-classification-using-keras/"},"\u201CPython Image Classification using Keras\u201D",-1),Jt=s(" and "),Qt=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720"},"\u201CTutorial on using Keras flow_from_directory and generators\u201D",-1),Xt=s(". The former allowed me to create and modify the hyper parameters of the model. While the latter allowed me to implement predictions so I could measure accuracy. "),Zt=s(" In the code above I'm getting a count of all the files to reconfirm the amount in each directory for each dataset. The dataset was split 60:20:20 Train:Validation:Test. "),ea=s(" In this snippet of code I set up the epochs which tells the model how many times to run through and the batch size. I then edited the number of layers and the number of node/parameters of each layer till my accuracy rose enough. Generally high epochs will increase the accuracy, but this is also prone to overfitting where the model is too tuned to the training that it can't identify other objects. In other words it can be too perfect. During testing this can hurt the accuracy of the model. "),ta=s(" At this point we are getting the images of the models and turning them into arrays that the program can understand. Then we fit the model with our training data and hyper parameters. "),aa=s(" Lastly we test our model and measure the accuracy of the model. Below is the output of the model with a layout of the layers and the result of running the dataset with this model. This model did suffer a little bit of over fitting, but I don't think it was enough to give bad results. "),sa=s("Source code for Support Vector Machines"),oa=s(" The code for this algorithm was from a tutorial on medium called, "),ia=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://medium.com/analytics-vidhya/image-classification-using-machine-learning-support-vector-machine-svm-dc7a0ec92e01"},"\u201CImage Classification Using Machine Learning-Support Vector Machine(SVM)\u201D",-1),na=s(". I modified the code to have it work with the dataset we are using for this dataset. In the code below I set up what the categories and directories being used were. Since their code using sklearn's train_test_split I had images merge together and then adjusted the test size on the function. In for loop you can see the dimensions of the image being resized to a set image. Since the code was using pixel intensities to measure, I treated the dimension sizes there as a hyper parameter and adjusted how many pixels would be used. "),ra=s(" After this we set up our SVM with a Gamma for influence, C for cost of misclassifications, and the Kernel. Afterwards we train our model and fit it. Then we start predicting images with our test data and record the accuracy. "),la=s(" Below is a result of each dimensions size I used as a hyper parameter to derive a better accuracy. "),ha=s(" In these graphs below you can see that as the dimensions of the images got bigger the training time dramatically increased, however the accuracy of the model did not. I felt that maybe since the features are pixels it would actually become too complex for the SVM to simply use distances. "),da=s(" In the final comparison of the algorithms I'll be using the 100 by 100 dimensions result. "),ca=s("Source code for K Nearest Neighbors"),ua=s(" The code for the algorithm was from a tutorial on medium called, "),ga=e("a",{style:{color:"#e07a5f"},target:"_blank",href:"https://medium.com/swlh/image-classification-with-k-nearest-neighbours-51b3a289280"},"\u201CImage Classification with K Nearest Neighbours\u201D",-1),ma=s(". I modified the code to have it work with the dataset we are using for this dataset. In the code below I set up the categories and once again mashed all of the data together since they were also using the sklearn's train_test_split and I simply adjusted the test size. In the load_image_files you can see a dimension parameter which I treated as a hyper parameter to achieve better accuracies. These images also treated pixel intensities as features. "),fa=s(" In this part of the code we are setting up the testing and training data and to be fitted into the KNeighborsClassifier model. "),pa=s(" At this point we are fitting the model and evaluating the score. In their code they wrote a for loop that would find the best K for the model and report that K with their accuracy. I extended the range to 200 to see if checking more Ks would possibly improve accuracy. "),_a=s(" Below you can see the results of each dimension I tried for the images. My theory was adding a greater dimension would increase the accuracy since more data would be utilized, however it seems the higher the dimension worse it got. "),ya=s(" I believe the results correlated with the same issue that SVM had with too many features being used therefore being too complex for SVM or KNN to be accurate. Additionally, you can see the more K increased the worse the model did. In the final comparison I used the 150 by 150 dimension result. "),wa=s("Performance Comparison"),ba=s(" Below is a comparison of each model below with their training times and accuracies. The Convolutional Neural Network proved to be the best for accuracy, but had a long training time. Did well behind CNN in second, but also had a very long training time. KNN came in last, but had a dramatically lesser training time. "),va=s(" I believe SVM and KNN suffered due to major over fitting of the models to their training data. They are designed to rely on the training data solely, but as the complexity or features increased the model quickly grew less accurate. CNN did suffer from over fitting, but not as bad as KNN and SVM. "),Ia=s("Pros & Cons For Each Classifier"),ka=s("s "),Ta=s("Convolutional Neural Networks"),xa=e("thead",null,[e("tr",null,[e("th",null,"Pros"),e("th",null,"Cons")])],-1),Na=e("tbody",null,[e("tr",null,[e("td",null,"Works well with complex objects for classification "),e("td",null,"Training Time can be long based on how many layers, epochs, and dataset size ")]),e("tr",null,[e("td",null,"Super effective with image classification "),e("td",null,"The algorithm is a little complex and needs strong math basis ")])],-1),$a=s("Support Vector Machines"),Ca=e("thead",null,[e("tr",null,[e("th",null,"Pros"),e("th",null,"Cons")])],-1),Ba=e("tbody",null,[e("tr",null,[e("td",null,"Faster training time than Convolutional Neural Networks "),e("td",null,"The algorithm becomes more complex as the dimensions increase ")]),e("tr",null,[e("td",null,"More accurate than KNN "),e("td",null,"Can't handle super complex / high number of features ")])],-1),Ka=s("K Nearest Neighbors"),Aa=e("thead",null,[e("tr",null,[e("th",null,"Pros"),e("th",null,"Cons")])],-1),ja=e("tbody",null,[e("tr",null,[e("td",null,"Super fast training time "),e("td",null,"Fails when object is super complex / features are high ")]),e("tr",null,[e("td",null,"Algorithm is very simple "),e("td",null,"Very Memory hungry ")]),e("tr",null,[e("td"),e("td",null,"Least Accurate ")])],-1),Sa=s("Contributions"),Ma=e("thead",null,[e("tr",null,[e("th",null,"Contribution"),e("th",null,"The value of the contribution"),e("th",null,"Technical Challenges")])],-1),za=e("tbody",null,[e("tr",null,[e("td",null,"Used regex to take off unneeded special characters"),e("td",null,"Allowed the data to be used for raw words that didn't have any extra unneeded information"),e("td",null,"Had to allow sum words to be missed since the the special characters such as single quotes needed to remain. I also had to learn regex and how to utilize it for the splitting. ")])],-1),Fa=s("References"),Da=e("thead",null,[e("tr",null,[e("th",null,"Reference"),e("th",null,"How I used the reference"),e("th",null,"What value I made over the reference")])],-1),Ea=e("tbody",null,[e("tr",null,[e("td",null,'"https://learn.microsoft.com/en-us/dotnet/standard/base-types/regular-expression-language-quick-reference" '),e("td",null,"I used this website to learn Regex "),e("td",null,'This allowed me to split my sentences using spaces and other delimiters for the "re" python library. ')])],-1),Ha=s("Repository:"),Va=e("a",{style:{color:"#e07a5f"},href:"https://github.com/ChristopherGordSmith/final_project_Data_mining",target:"_blank"}," This Blog's Repository",-1),Pa=s("Downloadable Code:"),Ra=e("a",{style:{color:"#e07a5f"},href:"assets/final-data-mining-project.ipynb",download:""}," CNN Classifier",-1),qa=e("a",{style:{color:"#e07a5f"},href:"assets/SVMImageClassifier.ipynb",download:""}," SVM Classifier",-1),Wa=e("a",{style:{color:"#e07a5f"},href:"assets/knn-classifier.ipynb",download:""}," KNN Classifier",-1);function Ga(m,f){const n=r("n-h1"),o=r("n-h2"),i=r("n-image"),l=r("n-h5"),h=r("n-table"),d=r("n-space"),c=r("n-gi"),_=r("n-grid");return u(),g(d,{vertical:""},{default:a(()=>[t(_,{cols:"1",responsive:"screen","x-gap":100},{default:a(()=>[t(c,null,{default:a(()=>[t(d,null,{default:a(()=>[t(n,{style:{color:"#e4572e"}},{default:a(()=>[Bt]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Kt,At,jt,St,Mt]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[zt]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Ft,Dt,Et]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[Ht]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Vt]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[Pt]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Rt]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[qt]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Wt,t(i,{style:{"margin-left":"30%","margin-right":"30%"},src:"assets/EuclideanDistance.png",width:"500"}),t(l,{style:{"margin-top":"0px",color:"white","margin-right":"44%","margin-left":"44%"}},{default:a(()=>[Gt]),_:1}),Lt]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[Ot]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Ut,Yt,Jt,Qt,Xt,t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/FinalCNNCode1.png",width:"500"}),Zt,t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/FinalCNNCode2.png",width:"500"}),ea,t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/FinalCNNCode3.png",width:"500"}),ta,t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/FinalCNNCode4.png",width:"500"}),aa,t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/FinalCNNCode5.png",width:"500"})]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[sa]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[oa,ia,na,t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/FinalSVMCode1.png",width:"500"}),ra,t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/FinalSVMCode2.png",width:"500"}),la,t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/100by100.png",width:"500"}),t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/100by100time.png",width:"500"}),t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/150by150.png",width:"500"}),t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/150by150time.png",width:"500"}),t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/fiftybyfifty.png",width:"500"}),t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/fiftybyfiftytime.png",width:"500"}),ha,t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/SVMTrainTime.png",width:"500"}),t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/SVMAccuracy.png",width:"500"}),da]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[ca]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[ua,ga,ma,t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/FinalKNNCode1.png",width:"500"}),fa,t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/FinalKNNCode2.png",width:"500"}),pa,t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/FinalKNNCode3.png",width:"500"}),_a,t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/KNN150by150.png",width:"500"}),t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/KNN200by200time.png",width:"500"}),t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/KNN250by250.png",width:"500"}),t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/KNNAccuracy.png",width:"500"}),ya]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[wa]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[ba,t(i,{style:{"margin-left":"35%","margin-right":"35%"},src:"assets/AlgorithmComparisons.png",width:"500"}),va]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[Ia]),_:1}),ka,t(o,{style:{"font-weight":"700"}},{default:a(()=>[t(o,{style:{color:"white"}},{default:a(()=>[Ta]),_:1}),t(h,{bordered:!1,"single-line":!1,style:{"margin-left":"20%","margin-right":"20%"}},{default:a(()=>[xa,Na]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[$a]),_:1}),t(h,{bordered:!1,"single-line":!1,style:{"margin-left":"20%","margin-right":"20%"}},{default:a(()=>[Ca,Ba]),_:1}),t(o,{style:{color:"white"}},{default:a(()=>[Ka]),_:1}),t(h,{bordered:!1,"single-line":!1,style:{"margin-left":"20%","margin-right":"20%"}},{default:a(()=>[Aa,ja]),_:1})]),_:1}),e("h2",null,[t(n,{style:{color:"#e4572e"}},{default:a(()=>[Sa]),_:1}),t(h,{bordered:!1,"single-line":!1},{default:a(()=>[Ma,za]),_:1})]),t(n,{style:{color:"#e4572e"}},{default:a(()=>[Fa]),_:1}),e("h2",null,[t(h,{bordered:!1,"single-line":!1},{default:a(()=>[Da,Ea]),_:1})]),t(n,{style:{color:"#e4572e"}},{default:a(()=>[Ha]),_:1}),t(o,null,{default:a(()=>[Va]),_:1}),t(n,{style:{color:"#e4572e"}},{default:a(()=>[Pa]),_:1}),t(o,null,{default:a(()=>[Ra,qa,Wa]),_:1})]),_:1})]),_:1})]),_:1})]),_:1})}const La=p(Ct,[["render",Ga]]),Oa=s("Kaggle Titanic Tutorial"),Ua=s("Image Classifier"),Ya=s("Naive Bayes Classifier"),Ja=s("Image Classifiers Comparison"),Xa=y({__name:"Blogs",setup(m){return(f,n)=>{const o=r("n-h1"),i=r("n-collapse-item"),l=r("n-collapse"),h=r("n-space");return u(),g(h,{style:{padding:"50px 100px 50px 100px"}},{default:a(()=>[t(l,{"default-expanded-names":"1",accordion:""},{default:a(()=>[t(i,null,{header:a(()=>[t(o,{style:{color:"#e4572e","font-size":"40px"}},{default:a(()=>[Oa]),_:1})]),default:a(()=>[t(B)]),_:1}),t(i,null,{header:a(()=>[t(o,{style:{color:"#e4572e","font-size":"40px"}},{default:a(()=>[Ua]),_:1})]),default:a(()=>[t(ke)]),_:1}),t(i,null,{header:a(()=>[t(o,{style:{color:"#e4572e","font-size":"40px"}},{default:a(()=>[Ya]),_:1})]),default:a(()=>[t($t)]),_:1}),t(i,null,{header:a(()=>[t(o,{style:{color:"#e4572e","font-size":"40px"}},{default:a(()=>[Ja]),_:1})]),default:a(()=>[t(La)]),_:1})]),_:1})]),_:1})}}});export{Xa as default};
